{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd0852d",
   "metadata": {},
   "source": [
    "# LLM-for-Metadata-Harvesting\n",
    "\n",
    "This notebook contains the experimental results from [P6: Groot zeegras (2023)](https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/TF1TbsTxTqykP5rv6MXJEg).  \n",
    "The results can be found under the last code block. Note that not all code is directly relevant to this experiment; some parts are retained for future development and elaboration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cheatsheet import CHEATSHEETS\n",
    "from prompt import PROMPTS\n",
    "from webutils import readWebContent, downloadAndParseXML\n",
    "\n",
    "dataPortalURL = [\n",
    "    \"https://developers.google.com/earth-engine/datasets/catalog/NASA_HLS_HLSS30_v002\",\n",
    "    \"https://lpdaac.usgs.gov/products/mod09a1v061/\",\n",
    "    \"https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en\",\n",
    "    \"https://stac.ecodatacube.eu/ndvi_glad.landsat.ard2.seasconv/collection.json?.language=en\",\n",
    "    \"https://zenodo.org/records/8319440\",\n",
    "    \"https://lifesciences.datastations.nl/dataset.xhtml?persistentId=doi:10.17026/dans-2bd-kskz\",\n",
    "    \"https://www.gbif.org/dataset/4fa7b334-ce0d-4e88-aaae-2e0c138d049e\",\n",
    "    \"https://www.gbif.org/dataset/74196cd9-7ebc-4b20-bc27-3c2d22e31ed7\",\n",
    "    \"https://www.gbif.org/dataset/f9ba3c2e-0636-4f66-a4b5-b8c138046e9e\",\n",
    "    \"https://www.gbif.org/dataset/bc0acb9a-131f-4085-93ae-a46e08564ac5\",\n",
    "    \"https://zenodo.org/records/11440456\",\n",
    "    \"https://stac.ecodatacube.eu/blue_glad.landsat.ard2.seasconv.m.yearly/collection.json\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/eng/catalog.search#/metadata/L-mHomzGRuKAHGMkUPjY9g\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/eng/catalog.search#/metadata/0fe7e64b-50b3-4cee-b64a-02659fc2b6c7\",\n",
    "    \"https://stac.ecodatacube.eu/green_glad.landsat.ard2.seasconv.m.yearly/collection.json\",\n",
    "    \"https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/A0h06_NlSEuNlium5OO3FA\",\n",
    "]\n",
    "\n",
    "# Get the web content\n",
    "url = dataPortalURL[0]\n",
    "\n",
    "\n",
    "# soup = readWebContent(url)\n",
    "# if soup is None:\n",
    "#     raise ValueError(\"Failed to retrieve web content\")\n",
    "\n",
    "# # Extract text from the webpage - adjust the selector based on the webpage structure\n",
    "# # This is a basic example - you might need to modify based on the specific webpage\n",
    "# text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "# text_xml, _ = downloadAndParseXML(\"https://datahuiswadden.openearth.nl/geonetwork/srv/api/records/A0h06_NlSEuNlium5OO3FA/formatters/xml\")\n",
    "# text += \"\\n\" + text_xml\n",
    "# full_text = text\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from webutils import extract_full_page_text\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async function\n",
    "full_text = await extract_full_page_text(url)\n",
    "\n",
    "# Optionally display or save it\n",
    "print(full_text)  # Print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from harvester_operations import extract_entities\n",
    "initial_url_map = {}\n",
    "clean_url_map = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for url in tqdm(dataPortalURL):\n",
    "    index += 1\n",
    "    # Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Run the async function\n",
    "    if url.startswith(\"https://lpdaac.usgs.gov\"):\n",
    "        soup = readWebContent(url)\n",
    "        if soup is None:\n",
    "            raise ValueError(\"Failed to retrieve web content\")\n",
    "\n",
    "        # Extract text from the webpage - adjust the selector based on the webpage structure\n",
    "        # This is a basic example - you might need to modify based on the specific webpage\n",
    "        full_text = soup.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        full_text = await extract_full_page_text(url)\n",
    "\n",
    "    # Optionally display or save it\n",
    "    ###############################################################\n",
    "    special_interest = CHEATSHEETS.get(\"special_interests\", \"Focus on metadata fields and their relationships\")\n",
    "    entity_types = PROMPTS[\"DEFAULT_ENTITY_TYPES\"]\n",
    "    is_croissant=False\n",
    "    # special_interest = CHEATSHEETS.get(\"special_interests_croissant\")\n",
    "    # entity_types = PROMPTS[\"CROISSANT_ENTITY_TYPES\"]\n",
    "    # is_croissant=True\n",
    "    ###############################################################\n",
    "\n",
    "    initial_nodes, clean_nodes = extract_entities(\n",
    "        text=full_text, \n",
    "        entity_types=entity_types, \n",
    "        special_interest=special_interest,\n",
    "        is_croissant=is_croissant,\n",
    "    )\n",
    "\n",
    "    initial_entity_type_map = {}\n",
    "\n",
    "    for entity_group in initial_nodes.values():\n",
    "        for item in entity_group:\n",
    "            entity_name = item.get('entity_name')\n",
    "            entity_type = item.get('entity_type')\n",
    "            entity_description = item.get('description')\n",
    "\n",
    "            # Initialize the list for this entity_type if not already present\n",
    "            if entity_name not in initial_entity_type_map:\n",
    "                initial_entity_type_map[entity_type] = []\n",
    "\n",
    "            # Append the (entity_name, description) pair\n",
    "            initial_entity_type_map[entity_type].append(entity_name + '; ' + entity_description)\n",
    "    initial_url_map[url] = initial_entity_type_map\n",
    "\n",
    "    # Create a dictionary to store entity_type: [(entity_name, description), ...]\n",
    "    clean_entity_type_map = {}\n",
    "\n",
    "    for entity_group in clean_nodes.values():\n",
    "        for item in entity_group:\n",
    "            entity_name = item.get('entity_name')\n",
    "            entity_value = item.get('entity_value')\n",
    "\n",
    "            # Initialize the list for this entity_type if not already present\n",
    "            if entity_name not in clean_entity_type_map:\n",
    "                clean_entity_type_map[entity_name] = []\n",
    "\n",
    "            # Append the (entity_name, description) pair\n",
    "            clean_entity_type_map[entity_name].append(entity_value)\n",
    "    clean_url_map[url] = clean_entity_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5af097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y-%m-%d\")\n",
    "prefix = \"cedar_openai_\"\n",
    "\n",
    "output_file_path = \"outputs/\" + date_str + \"/\" + prefix + \"clean_entity_type_map.yaml\"\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "# Sort the inner dictionary keys for each dataset URL\n",
    "sorted_data = {\n",
    "    url: dict(sorted(fields.items()))\n",
    "    for url, fields in clean_url_map.items()\n",
    "}\n",
    "\n",
    "# Save to YAML\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    yaml.dump(sorted_data, file, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "output_file_path = \"outputs/\" + date_str + \"/\" + prefix + \"initial_entity_type_map.yaml\"\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "# Sort the inner dictionary keys for each dataset URL\n",
    "sorted_data = {\n",
    "    url: dict(sorted(fields.items()))\n",
    "    for url, fields in initial_url_map.items()\n",
    "}\n",
    "\n",
    "# Save to YAML\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    yaml.dump(sorted_data, file, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Path to your YAML file\n",
    "input_file_path = \"outputs/2025-06-04/cedar_gemini_clean_entity_type_map.yaml\"\n",
    "\n",
    "# Load the YAML content\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    loaded_data = yaml.safe_load(file)\n",
    "\n",
    "# Now `loaded_data` is a Python dictionary\n",
    "print(loaded_data.keys())  # For example, show the top-level URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7941bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e05890",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11beba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
