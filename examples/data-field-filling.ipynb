{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd0852d",
   "metadata": {},
   "source": [
    "# LLM-for-Metadata-Harvesting\n",
    "\n",
    "This notebook demonstrates the use of Large Language Models (LLMs) for automated metadata extraction from web-based dataset portals.  \n",
    "It showcases an experiment on the [Actual probability distribution for Quercus robur (2000‚Äì2020)](https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en) dataset, using a combination of web scraping and LLM-based entity extraction to populate metadata fields according to the Croissant data standard.\n",
    "\n",
    "Key features:\n",
    "- Web scraping utilities for extracting full page text from dataset portals\n",
    "- Environment configuration for flexible API and model usage\n",
    "- LLM client support for OpenAI and Gemini models (with extensibility for custom clients)\n",
    "- Automated extraction of core metadata fields such as description, license, creator, keywords, and more\n",
    "\n",
    "The results of the experiment are presented at the end of the notebook.  \n",
    "Some code cells are included for future development and may not be directly relevant to this specific experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0968a41",
   "metadata": {},
   "source": [
    "# Scrap from the web portal of the dataset\n",
    "\n",
    "You can use own-defined function to get the data from website, or use the pre-defined function in webutils.\n",
    "\n",
    "Mind that you should check the `robots.txt` first to make sure if it is legal or allowed to scrap from this website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ccd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcoDataCub\n"
     ]
    }
   ],
   "source": [
    "from llm_metadata_harvester.webutils import extract_full_page_text\n",
    "import nest_asyncio\n",
    "\n",
    "url = \"https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en\"\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async function\n",
    "full_text = await extract_full_page_text(url)\n",
    "\n",
    "# Optionally display or save it\n",
    "print(full_text[:10])  # Print the first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48557b4",
   "metadata": {},
   "source": [
    "Alternatively, you can download the HTML of the webpage and read it as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf11b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./webpages/Actual probability distribution for Quercus robur (2000‚Äì2020).html\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    full_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177743a",
   "metadata": {},
   "source": [
    "## üîß Environment Configuration\n",
    "\n",
    "To configure your API keys or other environment variables, you can use a `.env` file or set them directly in your shell.\n",
    "\n",
    "### üìÑ Using a `.env` File\n",
    "\n",
    "Place the `.env` file in **one** of the following locations:\n",
    "\n",
    "- The **root directory** of your project  \n",
    "- The **same directory** as the script you're running  \n",
    "- Or any directory, **as long as it's the current working directory**\n",
    "\n",
    "> ‚ÑπÔ∏è The `load_dotenv()` function automatically looks for a `.env` file in the current working directory by default.\n",
    "\n",
    "#### üí° Example `.env` File\n",
    "```env\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "ANOTHER_SECRET=value_here\n",
    "```\n",
    "\n",
    "### Using global environment variable\n",
    "\n",
    "Alternatively, you can set environment variables directly in your shell:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450e1c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from llm_metadata_harvester.harvester_operations import extract_entities\n",
    "from llm_metadata_harvester.llm_client import LLMClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# can put your .env file in the root of the project\n",
    "# or in the same directory as this script\n",
    "# or set the environment variables directly in your shell\n",
    "# load_dotenv() will look for a .env file in the current directory\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b08d2",
   "metadata": {},
   "source": [
    "## Metadata Fields\n",
    "\n",
    "The metadata fields defined below follow the **Croissant data standard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667cec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metadata fields and their descriptions\n",
    "# These fields are from croissant data standard\n",
    "meta_field_dict = {\n",
    "    \"description\": \"Description of the dataset.\",\n",
    "    \"license\": \"The license of the dataset. Croissant recommends using the URL of a known license, e.g., one of the licenses listed at https://spdx.org/licenses/.\",\n",
    "    \"name\": \"The name of the dataset.\",\n",
    "    \"creator\": \"The creator(s) of the dataset.\",\n",
    "    \"datePublished\": \"The date the dataset was published.\",\n",
    "    \"keywords\": \"A set of keywords associated with the dataset, either as free text, or a DefinedTerm with a formal definition.\",\n",
    "    \"publisher\": \"The publisher of the dataset, which may be distinct from its creator.\",\n",
    "    \"sameAs\": \"The URL of another Web resource that represents the same dataset as this one.\",\n",
    "    \"dateModified\": \"The date the dataset was last modified.\",\n",
    "    \"inLanguage\": \"The language(s) of the content of the dataset.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af489ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metadata date': 'Specifies the date when the metadata record was created or last updated.;',\n",
       " 'Metadata language': 'Indicates the language used for the metadata descriptions.;',\n",
       " 'Responsible organization metadata': 'Identifies the organization or individual responsible for creating or managing the metadata record.;',\n",
       " 'Landing page': 'Provides the URL of a web page that serves as an entry point or primary access point for the resource.;',\n",
       " 'Title': 'Contains the name by which the resource is known.;',\n",
       " 'Description': 'Provides a narrative summary of the content and purpose of the resource.;',\n",
       " 'Unique Identifier': 'Provides a distinct code or name that unambiguously identifies the resource within a specific system.;',\n",
       " 'Resource type': 'Categorizes the nature or format of the resource, such as dataset, service, or publication.;',\n",
       " 'Keywords': 'Lists significant words or phrases that describe the topic or content of the resource.;',\n",
       " 'Data creator': 'Identifies the person or organization responsible for creating the dataset.;',\n",
       " 'Data contact point': 'Provides contact information for individuals or organizations responsible for the data.;',\n",
       " 'Data publisher': 'Identifies the organization or individual that makes the data available for public use.;',\n",
       " 'Spatial coverage': 'Describes the geographic area to which the resource applies.;',\n",
       " 'Spatial resolution': 'Specifies the level of detail or the smallest distinguishable unit in the spatial data.;',\n",
       " 'Spatial reference system': 'Indicates the coordinate system and projection used to define the spatial location of data.;',\n",
       " 'Temporal coverage': 'Describes the time period that the content of the resource represents or refers to.;',\n",
       " 'Temporal resolution': 'Indicates the frequency or interval at which data are collected or updated over time.;',\n",
       " 'License': 'Specifies the legal agreement governing the terms of use and distribution for the resource.;',\n",
       " 'Access rights': 'Describes the restrictions or permissions for accessing and using the resource.;',\n",
       " 'Distribution access URL': 'Provides a direct link to access or download the resource.;',\n",
       " 'Distribution format': 'Specifies the file format in which the resource is provided, such as CSV, GeoJSON, or XML.;',\n",
       " 'Distribution byte size': 'Indicates the size of the distributed resource in bytes.;'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_metadata_harvester.harvester_operations import metadata_harvest\n",
    "from llm_metadata_harvester.standards import LTER_LIFE_STANDARD\n",
    "\n",
    "extracted_metadata = await metadata_harvest(\n",
    "    model_name=\"gemini-2.5-flash-preview-05-20\",\n",
    "    url = \"https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en\",\n",
    "    metadata_standard=LTER_LIFE_STANDARD)\n",
    "\n",
    "extracted_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef2b60",
   "metadata": {},
   "source": [
    "## LLM Client Support\n",
    "\n",
    "The `llm client` currently supports **OpenAI** and **Gemini** models.\n",
    "\n",
    "To use other models, you can define your own LLM client class.  \n",
    "Your custom class should implement a `chat` method that returns a string as the LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLMClient(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "clean_nodes = extract_entities(\n",
    "    text=full_text,\n",
    "    meta_field_dict=meta_field_dict,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85390a84",
   "metadata": {},
   "source": [
    "The output of `extract_entities` looks like a list of lists of dictionaries, where each dictionary is structured like this:\n",
    "\n",
    "```python\n",
    "'license': [{'entity_name': 'license',\n",
    "             'entity_value': 'CC-BY-SA-4.0',\n",
    "             'source_id': 'chunk_0',\n",
    "             'file_path': 'unknown_source'}]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
