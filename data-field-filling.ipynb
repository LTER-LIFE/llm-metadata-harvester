{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd0852d",
   "metadata": {},
   "source": [
    "# LLM-for-Metadata-Harvesting\n",
    "\n",
    "This notebook demonstrates the use of Large Language Models (LLMs) for automated metadata extraction from web-based dataset portals.  \n",
    "It showcases an experiment on the [Actual probability distribution for Quercus robur (2000‚Äì2020)](https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en) dataset, using a combination of web scraping and LLM-based entity extraction to populate metadata fields according to the Croissant data standard.\n",
    "\n",
    "Key features:\n",
    "- Web scraping utilities for extracting full page text from dataset portals\n",
    "- Environment configuration for flexible API and model usage\n",
    "- LLM client support for OpenAI and Gemini models (with extensibility for custom clients)\n",
    "- Automated extraction of core metadata fields such as description, license, creator, keywords, and more\n",
    "\n",
    "The results of the experiment are presented at the end of the notebook.  \n",
    "Some code cells are included for future development and may not be directly relevant to this specific experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0968a41",
   "metadata": {},
   "source": [
    "# Scrap from the web portal of the dataset\n",
    "\n",
    "You can use own-defined function to get the data from website, or use the pre-defined function in webutils.\n",
    "\n",
    "Mind that you should check the `robots.txt` first to make sure if it is legal or allowed to scrap from this website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35ccd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcoDataCub\n"
     ]
    }
   ],
   "source": [
    "from llm_metadata_harvester.webutils import extract_full_page_text\n",
    "import nest_asyncio\n",
    "\n",
    "url = \"https://stac.ecodatacube.eu/veg_quercus.robur_anv.eml/collection.json?.language=en\"\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio.run() in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async function\n",
    "full_text = await extract_full_page_text(url)\n",
    "\n",
    "# Optionally display or save it\n",
    "print(full_text[:10])  # Print the first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177743a",
   "metadata": {},
   "source": [
    "## üîß Environment Configuration\n",
    "\n",
    "To configure your API keys or other environment variables, you can use a `.env` file or set them directly in your shell.\n",
    "\n",
    "### üìÑ Using a `.env` File\n",
    "\n",
    "Place the `.env` file in **one** of the following locations:\n",
    "\n",
    "- The **root directory** of your project  \n",
    "- The **same directory** as the script you're running  \n",
    "- Or any directory, **as long as it's the current working directory**\n",
    "\n",
    "> ‚ÑπÔ∏è The `load_dotenv()` function automatically looks for a `.env` file in the current working directory by default.\n",
    "\n",
    "#### üí° Example `.env` File\n",
    "```env\n",
    "OPENAI_API_KEY=your_api_key_here\n",
    "ANOTHER_SECRET=value_here\n",
    "```\n",
    "\n",
    "### Using global environment variable\n",
    "\n",
    "Alternatively, you can set environment variables directly in your shell:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450e1c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from llm_metadata_harvester.harvester_operations import extract_entities\n",
    "from llm_metadata_harvester.llm_client import LLMClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# can put your .env file in the root of the project\n",
    "# or in the same directory as this script\n",
    "# or set the environment variables directly in your shell\n",
    "# load_dotenv() will look for a .env file in the current directory\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b08d2",
   "metadata": {},
   "source": [
    "## Metadata Fields\n",
    "\n",
    "The metadata fields defined below follow the **Croissant data standard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667cec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metadata fields and their descriptions\n",
    "# These fields are from croissant data standard\n",
    "meta_field_dict = {\n",
    "    \"description\": \"Description of the dataset.\",\n",
    "    \"license\": \"The license of the dataset. Croissant recommends using the URL of a known license, e.g., one of the licenses listed at https://spdx.org/licenses/.\",\n",
    "    \"name\": \"The name of the dataset.\",\n",
    "    \"creator\": \"The creator(s) of the dataset.\",\n",
    "    \"datePublished\": \"The date the dataset was published.\",\n",
    "    \"keywords\": \"A set of keywords associated with the dataset, either as free text, or a DefinedTerm with a formal definition.\",\n",
    "    \"publisher\": \"The publisher of the dataset, which may be distinct from its creator.\",\n",
    "    \"sameAs\": \"The URL of another Web resource that represents the same dataset as this one.\",\n",
    "    \"dateModified\": \"The date the dataset was last modified.\",\n",
    "    \"inLanguage\": \"The language(s) of the content of the dataset.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef2b60",
   "metadata": {},
   "source": [
    "## LLM Client Support\n",
    "\n",
    "The `llm client` currently supports **OpenAI** and **Gemini** models.\n",
    "\n",
    "To use other models, you can define your own LLM client class.  \n",
    "Your custom class should implement a `chat` method that returns a string as the LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b7232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLMClient(model_name=\"gemini-2.5-flash-preview-05-20\", temperature=0.0)\n",
    "\n",
    "clean_nodes = extract_entities(\n",
    "    text=full_text,\n",
    "    meta_field_dict=meta_field_dict,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85390a84",
   "metadata": {},
   "source": [
    "The output of `extract_entities` looks like a list of lists of dictionaries, where each dictionary is structured like this:\n",
    "\n",
    "```python\n",
    "'license': [{'entity_name': 'license',\n",
    "             'entity_value': 'CC-BY-SA-4.0',\n",
    "             'source_id': 'chunk_0',\n",
    "             'file_path': 'unknown_source'}]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
